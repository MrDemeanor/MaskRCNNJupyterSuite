{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "### This notebook runs the COCO evaluation tool and prints out accuracy metrics pertaining to bounding box accuracy and/or segmentation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# COCO related libraries\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "from samples.coco import coco\n",
    "from samples.coco.coco import evaluate_coco\n",
    "\n",
    "# MaskRCNN libraries\n",
    "import mrcnn.model as modellib\n",
    "import mrcnn.utils as utils\n",
    "import mrcnn.visualize as visualize\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import skimage.io\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in dataset. Must be of type integer\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# Relative path to .h5 weights file\n",
    "WEIGHTS_FILE = \"logs/TrainingSequencebeforeSmallAreaPruning/maskrcnn-masklayerscount:420190719T1316/mask_rcnn_maskrcnn-masklayerscount:4_0040.h5\"\n",
    "\n",
    "# Relative path to ground truth annotations JSON file\n",
    "GT_ANNOTATIONS_FILE = \"datasets/Downtown_Sliced/test/annotations.json\"\n",
    "\n",
    "# Relative path to images associated with ground truth JSON file\n",
    "DATASET_DIR = \"datasets/Downtown_Sliced/test/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Select which GPU to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare evaluation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalConfig(coco.CocoConfig):\n",
    "    \"\"\" Configuration for evaluation \"\"\"\n",
    "    \n",
    "    # How many GPUs\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    # How many images per gpu\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + NUM_CLASSES  # background + 1 (structure)\n",
    "\n",
    "    # All of our training images are 300x300\n",
    "    IMAGE_MIN_DIM = 1152\n",
    "    IMAGE_MAX_DIM = 1280\n",
    "    \n",
    "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    BACKBONE = 'resnet101'\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    \n",
    "    # Changed to 512 because that's how many the original MaskRCNN paper used\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "    MAX_GT_INSTANCES = 114\n",
    "    POST_NMS_ROIS_INFERENCE = 1000 \n",
    "    POST_NMS_ROIS_TRAINING = 2000 \n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 114\n",
    "    DETECTION_MIN_CONFIDENCE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        114\n",
      "DETECTION_MIN_CONFIDENCE       0.1\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1280\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  1152\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1280 1280    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               114\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           coco\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EvalConfig().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build class to load ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(utils.Dataset):\n",
    "    def load_coco_gt(self, annotations_file, dataset_dir):\n",
    "        \"\"\"Load a COCO styled ground truth dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create COCO object\n",
    "        coco = COCO(annotations_file)\n",
    "\n",
    "        # Load all classes\n",
    "        class_ids = sorted(coco.getCatIds())\n",
    "\n",
    "        # Load all images\n",
    "        image_ids = list(coco.imgs.keys())\n",
    "\n",
    "        # Add classes\n",
    "        for i in class_ids:\n",
    "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
    "\n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"coco\", image_id = i,\n",
    "                path = os.path.join(dataset_dir, coco.imgs[i]['file_name']),\n",
    "                width = coco.imgs[i][\"width\"],\n",
    "                height = coco.imgs[i][\"height\"],\n",
    "                annotations = coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds = [i], catIds = class_ids, iscrowd=None)))\n",
    "        \n",
    "        return coco\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"coco\":\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"coco\":\n",
    "            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n",
    "        else:\n",
    "            super(CocoDataset, self).image_reference(image_id)\n",
    "\n",
    "    # The following two functions are from pycocotools with a few changes.\n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 07:57:04.003538 139822599010112 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0726 07:57:04.022788 139822599010112 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0726 07:57:04.027951 139822599010112 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0726 07:57:04.079143 139822599010112 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0726 07:57:08.219020 139822599010112 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0726 07:57:08.862669 139822599010112 deprecation_wrapper.py:119] From /home/venv/src/mask-rcnn/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0726 07:57:08.873883 139822599010112 deprecation.py:323] From /home/venv/src/mask-rcnn/mrcnn/model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0726 07:57:08.881486 139822599010112 deprecation.py:506] From /home/venv/src/mask-rcnn/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W0726 07:57:09.186199 139822599010112 deprecation_wrapper.py:119] From /home/venv/src/mask-rcnn/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "W0726 07:57:09.191326 139822599010112 deprecation_wrapper.py:119] From /home/venv/src/mask-rcnn/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "W0726 07:57:09.303579 139822599010112 deprecation.py:323] From /home/venv/src/mask-rcnn/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", config = EvalConfig(), model_dir = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights by name\n",
    "model.load_weights(WEIGHTS_FILE, by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=19.06s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.13s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Prediction time: 951.8477532863617. Average 3.3515765960787385/image\n",
      "Total time:  1108.7217586040497\n"
     ]
    }
   ],
   "source": [
    "dataset_val = CocoDataset()\n",
    "coco = dataset_val.load_coco_gt(annotations_file = GT_ANNOTATIONS_FILE, dataset_dir = DATASET_DIR)\n",
    "dataset_val.prepare()\n",
    "evaluate_coco(model, dataset_val, coco, \"segm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
