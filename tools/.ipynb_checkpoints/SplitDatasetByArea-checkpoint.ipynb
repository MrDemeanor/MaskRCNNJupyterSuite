{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1a89755150a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplitter():\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            - annotations_file: \n",
    "    \"\"\"\n",
    "    def __init__(self, annotations_file, annotations_output_dir, categories_list, boundaries_list):\n",
    "        \n",
    "        # Preliminary error checking\n",
    "        assert os.path.isfile(os.getcwd() + \"/\" + annotations_file), \"Annotations file at path {} does not exist\".format(annotations_file)\n",
    "        assert os.path.isdir(os.getcwd() + \"/\" + annotations_output_dir), \"Annotations output dir at path {} does not exist\".format(annotations_output_dir)\n",
    "        assert categories_list is not None, \"Categories list must be populated\"\n",
    "        assert boundaries_list is not None, \"Boundaries list must be populated\"\n",
    "        assert len(categories_list) == len(boundaries_list), \"Boundary list must be the same size as the categories list\"\n",
    "        \n",
    "        # Assigning variables\n",
    "        self.coco_json = COCO(annotations_file)\n",
    "        self.annotations_output_dir = annotations_output_dir\n",
    "        self.categories_list = categories_list\n",
    "        self.boundaries_list = boundaries_list\n",
    "        \n",
    "        # Get image IDs for all images in dataset\n",
    "        self.imgIds = self.coco_json.getImgIds()\n",
    "        self.images = self.coco_json.loadImgs(ids = self.imgIds)\n",
    "    \n",
    "    def splitDataset(self):\n",
    "        \n",
    "        # Make coco json master list\n",
    "        split_coco_json = {\n",
    "            \"images\": [], \n",
    "            \"annotations\": [], \n",
    "            \"categories\": self.categories_list\n",
    "        }\n",
    "        \n",
    "        # Add images to image list\n",
    "        for image in self.images:\n",
    "            split_coco_json[\"images\"].append(image)\n",
    "        \n",
    "        # Process each annotation in each image\n",
    "        for x in tnrange(len(self.imgIds), desc = 'Processing image annotations...'):\n",
    "            \n",
    "            # Get annotation IDs pertaining to image\n",
    "            annIds = self.coco_json.getAnnIds(imgIds = self.imgIds[x])\n",
    "\n",
    "            # Get all annotations pertaining to image\n",
    "            annotations = self.coco_json.loadAnns(ids = annIds)\n",
    "            \n",
    "            for annotation in annotations:\n",
    "                \n",
    "                # Make a copy of boundaries list\n",
    "                temp_list = copy.deepcopy(self.boundaries_list)\n",
    "                \n",
    "                # Add area to temp list, then sort and find index of area\n",
    "                temp_list.append(annotation[\"area\"])\n",
    "                sorted_list = sorted(temp_list)\n",
    "                index = sorted_list.index(annotation[\"area\"]) + 1\n",
    "                \n",
    "                # Change category id to index and add to split coco json master dictionary\n",
    "                annotation[\"category_id\"] = index\n",
    "                split_coco_json[\"annotations\"].append(annotation)\n",
    "        \n",
    "        with open(self.annotations_output_dir + \"/annotations_split.json\", \"w\") as outfile:\n",
    "            json.dump(split_coco_json, outfile)\n",
    "        \n",
    "        print(\"Saved annotations file to {}\".format(self.annotations_output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.28s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories_list = [\n",
    "    \n",
    "    # Area 166 - 50,000\n",
    "    {\n",
    "        \"id\": 1, \n",
    "        \"name\": \"Small Structure\", \n",
    "        \"supercategory\": \"Structure\"\n",
    "    }, \n",
    "    \n",
    "    # Area 50,001 - 200,000\n",
    "    {\n",
    "        \"id\": 2, \n",
    "        \"name\": \"Medium Structure\", \n",
    "        \"supercategory\": \"Structure\"\n",
    "    }, \n",
    "    \n",
    "    # Area 200,001 and above\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"Other\", \n",
    "        \"supercategory\": \"Structure\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset_splitter = DatasetSplitter(    annotations_file = \"datasets/Downtown_Sliced/test/annotations.json\", \n",
    "                                       annotations_output_dir = \"datasets/Downtown_Sliced/test\", \n",
    "                                       categories_list = categories_list,\n",
    "                                       boundaries_list = [50000, 200000, 900000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c1ca770ebe497f8e5b3ad8beadb750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Processing image annotations...', max=284, style=ProgressStylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved annotations file to datasets/Downtown_Sliced/test\n"
     ]
    }
   ],
   "source": [
    "dataset_splitter.splitDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
