{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# COCO related libraries\n",
    "from samples.coco import coco\n",
    "\n",
    "# MaskRCNN libraries\n",
    "from mrcnn.config import Config\n",
    "import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "import mrcnn.model as modellib\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in dataset. Must be of type integer\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# Relative path to .h5 weights file\n",
    "WEIGHTS_FILE = None\n",
    "\n",
    "# Relative path to annotations JSON file\n",
    "TRAIN_ANNOTATIONS_FILE = \"datasets/Downtown_Sliced/train/annotations_split.json\"\n",
    "\n",
    "# Relative path to directory of images that pertain to annotations file\n",
    "TRAIN_ANNOTATION_IMAGE_DIR = 'datasets/Downtown_Sliced/train/images'\n",
    "\n",
    "# Relative path to annotations JSON file\n",
    "VALIDATION_ANNOTATIONS_FILE = \"datasets/Downtown_Sliced/validation/annotations_split.json\"\n",
    "\n",
    "# Relative path to directory of images that pertain to annotations file\n",
    "VALIDATION_ANNOTATION_IMAGE_DIR = 'datasets/Downtown_Sliced/validation/images'\n",
    "\n",
    "# Number of epochs to train dataset on\n",
    "NUM_EPOCHS = 80\n",
    "\n",
    "MODEL_NAME = \"model_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Select which GPU to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig(coco.CocoConfig):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = MODEL_NAME\n",
    "\n",
    "    # Train on 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + NUM_CLASSES\n",
    "\n",
    "    # Min and max image dimensions\n",
    "    IMAGE_MIN_DIM = 1152\n",
    "    IMAGE_MAX_DIM = 1280\n",
    "\n",
    "    # You can experiment with this number to see if it improves training\n",
    "    STEPS_PER_EPOCH = 180\n",
    "\n",
    "    # This is how often validation is run. If you are using too much hard drive space\n",
    "    # on saved models (in the MODEL_DIR), try making this value larger.\n",
    "    VALIDATION_STEPS = 50\n",
    "    \n",
    "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    BACKBONE = 'resnet101'\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    \n",
    "    # Changed to 512 because that's how many the original MaskRCNN paper used\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "    MAX_GT_INSTANCES = 114\n",
    "    POST_NMS_ROIS_INFERENCE = 1000 \n",
    "    POST_NMS_ROIS_TRAINING = 2000 \n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 114\n",
    "    DETECTION_MIN_CONFIDENCE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        114\n",
      "DETECTION_MIN_CONFIDENCE       0.1\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1280\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  1152\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1280 1280    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               114\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           model_2\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                180\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TrainConfig().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class to load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoLikeDataset(utils.Dataset):\n",
    "    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n",
    "        See http://cocodataset.org/#home for more information.\n",
    "    \"\"\"\n",
    "    def load_data(self, annotation_json, images_dir):\n",
    "        \"\"\" Load the coco-like dataset from json\n",
    "        Args:\n",
    "            annotation_json: The path to the coco annotations json file\n",
    "            images_dir: The directory holding the images referred to by the json file\n",
    "        \"\"\"\n",
    "        # Load json from file\n",
    "        json_file = open(annotation_json)\n",
    "        coco_json = json.load(json_file)\n",
    "        json_file.close()\n",
    "        \n",
    "        # Add the class names using the base method from utils.Dataset\n",
    "        source_name = \"coco_like\"\n",
    "        for category in coco_json['categories']:\n",
    "            class_id = category['id']\n",
    "            class_name = category['name']\n",
    "            if class_id < 1:\n",
    "                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n",
    "                return\n",
    "            \n",
    "            self.add_class(source_name, class_id, class_name)\n",
    "        \n",
    "        # Get all annotations\n",
    "        annotations = {}\n",
    "        for annotation in coco_json['annotations']:\n",
    "            image_id = annotation['image_id']\n",
    "            if image_id not in annotations:\n",
    "                annotations[image_id] = []\n",
    "            annotations[image_id].append(annotation)\n",
    "        \n",
    "        # Get all images and add them to the dataset\n",
    "        seen_images = {}\n",
    "        for image in coco_json['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in seen_images:\n",
    "                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                seen_images[image_id] = image\n",
    "                try:\n",
    "                    image_file_name = image['file_name']\n",
    "                    image_width = image['width']\n",
    "                    image_height = image['height']\n",
    "                except KeyError as key:\n",
    "                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n",
    "                \n",
    "                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n",
    "                image_annotations = annotations[image_id]\n",
    "                \n",
    "                # Add the image using the base method from utils.Dataset\n",
    "                self.add_image(\n",
    "                    source=source_name,\n",
    "                    image_id=image_id,\n",
    "                    path=image_path,\n",
    "                    width=image_width,\n",
    "                    height=image_height,\n",
    "                    annotations=image_annotations\n",
    "                )\n",
    "                \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\" Load instance masks for the given image.\n",
    "        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n",
    "        Args:\n",
    "            image_id: The id of the image to load masks for\n",
    "        Returns:\n",
    "            masks: A bool array of shape [height, width, instance count] with\n",
    "                one mask per instance.\n",
    "            class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        image_info = self.image_info[image_id]\n",
    "        annotations = image_info['annotations']\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            class_id = annotation['category_id']\n",
    "            mask = Image.new('1', (image_info['width'], image_info['height']))\n",
    "            mask_draw = ImageDraw.ImageDraw(mask, '1')\n",
    "            for segmentation in annotation['segmentation']:\n",
    "                mask_draw.polygon(segmentation, fill=1)\n",
    "                bool_array = np.array(mask) > 0\n",
    "                instance_masks.append(bool_array)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        mask = np.dstack(instance_masks)\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        \n",
    "        return mask, class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CocoLikeDataset()\n",
    "dataset_train.load_data(TRAIN_ANNOTATIONS_FILE, TRAIN_ANNOTATION_IMAGE_DIR)\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_val = CocoLikeDataset()\n",
    "dataset_val.load_data(VALIDATION_ANNOTATIONS_FILE, VALIDATION_ANNOTATION_IMAGE_DIR)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 20:55:55.028046 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0805 20:55:55.066967 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0805 20:55:55.108044 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0805 20:55:55.145821 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0805 20:56:01.127417 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0805 20:56:01.878015 139871543367488 deprecation.py:323] From /home/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0805 20:56:02.051718 139871543367488 deprecation_wrapper.py:119] From /home/venv/src/mask-rcnn/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "W0805 20:56:02.144878 139871543367488 deprecation_wrapper.py:119] From /home/venv/src/mask-rcnn/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0805 20:56:02.175616 139871543367488 deprecation.py:506] From /home/venv/src/mask-rcnn/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode = \"training\", config = TrainConfig(), model_dir = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights into model if weights file is not None\n",
    "### This is meant to be used if you are refining on a set of preexisting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WEIGHTS_FILE is not None:\n",
    "    model.load_weights(WEIGHTS_FILE, by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### The model after each epoch will be saved in the logs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/logs/model_220190805T2055/mask_rcnn_model_2_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 20:56:06.696414 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "/home/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/venv/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n",
      "W0805 20:56:42.665176 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/callbacks.py:783: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0805 20:56:42.667382 139871543367488 deprecation_wrapper.py:119] From /home/venv/lib/python3.6/site-packages/keras/callbacks.py:786: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "179/180 [============================>.] - ETA: 1s - loss: 5.9014 - rpn_class_loss: 1.3325 - rpn_bbox_loss: 2.6158 - mrcnn_class_loss: 0.1858 - mrcnn_bbox_loss: 1.0787 - mrcnn_mask_loss: 0.6884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venv/lib/python3.6/site-packages/keras/engine/training.py:2348: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 269s 1s/step - loss: 5.8876 - rpn_class_loss: 1.3288 - rpn_bbox_loss: 2.6091 - mrcnn_class_loss: 0.1854 - mrcnn_bbox_loss: 1.0758 - mrcnn_mask_loss: 0.6884 - val_loss: 4.0981 - val_rpn_class_loss: 0.6354 - val_rpn_bbox_loss: 1.8011 - val_mrcnn_class_loss: 0.3238 - val_mrcnn_bbox_loss: 0.6641 - val_mrcnn_mask_loss: 0.6737\n",
      "Epoch 2/80\n",
      "180/180 [==============================] - 234s 1s/step - loss: 3.4330 - rpn_class_loss: 0.5944 - rpn_bbox_loss: 1.3371 - mrcnn_class_loss: 0.2382 - mrcnn_bbox_loss: 0.6185 - mrcnn_mask_loss: 0.6448 - val_loss: 3.3771 - val_rpn_class_loss: 0.5592 - val_rpn_bbox_loss: 1.2950 - val_mrcnn_class_loss: 0.2990 - val_mrcnn_bbox_loss: 0.6195 - val_mrcnn_mask_loss: 0.6044\n",
      "Epoch 3/80\n",
      "180/180 [==============================] - 221s 1s/step - loss: 2.9889 - rpn_class_loss: 0.5030 - rpn_bbox_loss: 1.0623 - mrcnn_class_loss: 0.3170 - mrcnn_bbox_loss: 0.5020 - mrcnn_mask_loss: 0.6045 - val_loss: 3.1165 - val_rpn_class_loss: 0.4647 - val_rpn_bbox_loss: 1.2767 - val_mrcnn_class_loss: 0.2443 - val_mrcnn_bbox_loss: 0.5497 - val_mrcnn_mask_loss: 0.5810\n",
      "Epoch 4/80\n",
      "180/180 [==============================] - 231s 1s/step - loss: 2.8717 - rpn_class_loss: 0.4479 - rpn_bbox_loss: 1.1373 - mrcnn_class_loss: 0.2825 - mrcnn_bbox_loss: 0.4717 - mrcnn_mask_loss: 0.5323 - val_loss: 3.5042 - val_rpn_class_loss: 0.5421 - val_rpn_bbox_loss: 1.4405 - val_mrcnn_class_loss: 0.3525 - val_mrcnn_bbox_loss: 0.6175 - val_mrcnn_mask_loss: 0.5515\n",
      "Epoch 5/80\n",
      "180/180 [==============================] - 234s 1s/step - loss: 2.8659 - rpn_class_loss: 0.4260 - rpn_bbox_loss: 1.1384 - mrcnn_class_loss: 0.3165 - mrcnn_bbox_loss: 0.4839 - mrcnn_mask_loss: 0.5011 - val_loss: 2.9671 - val_rpn_class_loss: 0.4398 - val_rpn_bbox_loss: 1.1090 - val_mrcnn_class_loss: 0.2749 - val_mrcnn_bbox_loss: 0.5838 - val_mrcnn_mask_loss: 0.5595\n",
      "Epoch 6/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 2.5980 - rpn_class_loss: 0.3926 - rpn_bbox_loss: 0.9383 - mrcnn_class_loss: 0.3369 - mrcnn_bbox_loss: 0.4598 - mrcnn_mask_loss: 0.4704 - val_loss: 3.0229 - val_rpn_class_loss: 0.4087 - val_rpn_bbox_loss: 1.2650 - val_mrcnn_class_loss: 0.2722 - val_mrcnn_bbox_loss: 0.5508 - val_mrcnn_mask_loss: 0.5261\n",
      "Epoch 7/80\n",
      "180/180 [==============================] - 230s 1s/step - loss: 2.6848 - rpn_class_loss: 0.3741 - rpn_bbox_loss: 1.1406 - mrcnn_class_loss: 0.3327 - mrcnn_bbox_loss: 0.4020 - mrcnn_mask_loss: 0.4353 - val_loss: 3.0653 - val_rpn_class_loss: 0.4148 - val_rpn_bbox_loss: 1.1550 - val_mrcnn_class_loss: 0.3829 - val_mrcnn_bbox_loss: 0.5326 - val_mrcnn_mask_loss: 0.5800\n",
      "Epoch 8/80\n",
      "180/180 [==============================] - 231s 1s/step - loss: 2.5822 - rpn_class_loss: 0.3850 - rpn_bbox_loss: 0.9633 - mrcnn_class_loss: 0.3430 - mrcnn_bbox_loss: 0.4398 - mrcnn_mask_loss: 0.4511 - val_loss: 2.8417 - val_rpn_class_loss: 0.3751 - val_rpn_bbox_loss: 1.0904 - val_mrcnn_class_loss: 0.3116 - val_mrcnn_bbox_loss: 0.5199 - val_mrcnn_mask_loss: 0.5447\n",
      "Epoch 9/80\n",
      "180/180 [==============================] - 227s 1s/step - loss: 2.4352 - rpn_class_loss: 0.3309 - rpn_bbox_loss: 0.9508 - mrcnn_class_loss: 0.3124 - mrcnn_bbox_loss: 0.4081 - mrcnn_mask_loss: 0.4329 - val_loss: 2.8789 - val_rpn_class_loss: 0.3998 - val_rpn_bbox_loss: 1.2977 - val_mrcnn_class_loss: 0.2003 - val_mrcnn_bbox_loss: 0.4809 - val_mrcnn_mask_loss: 0.5003\n",
      "Epoch 10/80\n",
      "180/180 [==============================] - 229s 1s/step - loss: 2.4661 - rpn_class_loss: 0.3167 - rpn_bbox_loss: 1.0408 - mrcnn_class_loss: 0.2957 - mrcnn_bbox_loss: 0.3939 - mrcnn_mask_loss: 0.4189 - val_loss: 2.9951 - val_rpn_class_loss: 0.4779 - val_rpn_bbox_loss: 1.0887 - val_mrcnn_class_loss: 0.3924 - val_mrcnn_bbox_loss: 0.5154 - val_mrcnn_mask_loss: 0.5207\n",
      "Epoch 11/80\n",
      "180/180 [==============================] - 224s 1s/step - loss: 2.5182 - rpn_class_loss: 0.3329 - rpn_bbox_loss: 1.0794 - mrcnn_class_loss: 0.3128 - mrcnn_bbox_loss: 0.3810 - mrcnn_mask_loss: 0.4122 - val_loss: 2.8322 - val_rpn_class_loss: 0.3999 - val_rpn_bbox_loss: 1.1628 - val_mrcnn_class_loss: 0.2866 - val_mrcnn_bbox_loss: 0.4816 - val_mrcnn_mask_loss: 0.5013\n",
      "Epoch 12/80\n",
      "180/180 [==============================] - 233s 1s/step - loss: 2.3727 - rpn_class_loss: 0.3216 - rpn_bbox_loss: 0.9143 - mrcnn_class_loss: 0.3346 - mrcnn_bbox_loss: 0.3860 - mrcnn_mask_loss: 0.4161 - val_loss: 2.6087 - val_rpn_class_loss: 0.3741 - val_rpn_bbox_loss: 1.0571 - val_mrcnn_class_loss: 0.1700 - val_mrcnn_bbox_loss: 0.4881 - val_mrcnn_mask_loss: 0.5193\n",
      "Epoch 13/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 2.4806 - rpn_class_loss: 0.3190 - rpn_bbox_loss: 1.0622 - mrcnn_class_loss: 0.3225 - mrcnn_bbox_loss: 0.3756 - mrcnn_mask_loss: 0.4013 - val_loss: 2.9195 - val_rpn_class_loss: 0.3728 - val_rpn_bbox_loss: 1.1883 - val_mrcnn_class_loss: 0.3611 - val_mrcnn_bbox_loss: 0.4909 - val_mrcnn_mask_loss: 0.5063\n",
      "Epoch 14/80\n",
      "180/180 [==============================] - 243s 1s/step - loss: 2.2112 - rpn_class_loss: 0.2811 - rpn_bbox_loss: 0.8353 - mrcnn_class_loss: 0.3002 - mrcnn_bbox_loss: 0.3865 - mrcnn_mask_loss: 0.4081 - val_loss: 2.5338 - val_rpn_class_loss: 0.4314 - val_rpn_bbox_loss: 0.9601 - val_mrcnn_class_loss: 0.1686 - val_mrcnn_bbox_loss: 0.4852 - val_mrcnn_mask_loss: 0.4884\n",
      "Epoch 15/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 2.3398 - rpn_class_loss: 0.2804 - rpn_bbox_loss: 0.9697 - mrcnn_class_loss: 0.3214 - mrcnn_bbox_loss: 0.3727 - mrcnn_mask_loss: 0.3957 - val_loss: 2.7782 - val_rpn_class_loss: 0.3817 - val_rpn_bbox_loss: 1.1843 - val_mrcnn_class_loss: 0.2123 - val_mrcnn_bbox_loss: 0.4808 - val_mrcnn_mask_loss: 0.5190\n",
      "Epoch 16/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 2.2464 - rpn_class_loss: 0.2812 - rpn_bbox_loss: 0.8831 - mrcnn_class_loss: 0.3027 - mrcnn_bbox_loss: 0.3738 - mrcnn_mask_loss: 0.4056 - val_loss: 2.8764 - val_rpn_class_loss: 0.3957 - val_rpn_bbox_loss: 1.0777 - val_mrcnn_class_loss: 0.3325 - val_mrcnn_bbox_loss: 0.5374 - val_mrcnn_mask_loss: 0.5331\n",
      "Epoch 17/80\n",
      "180/180 [==============================] - 239s 1s/step - loss: 2.2030 - rpn_class_loss: 0.2677 - rpn_bbox_loss: 0.8853 - mrcnn_class_loss: 0.3199 - mrcnn_bbox_loss: 0.3472 - mrcnn_mask_loss: 0.3830 - val_loss: 2.5943 - val_rpn_class_loss: 0.3554 - val_rpn_bbox_loss: 1.0019 - val_mrcnn_class_loss: 0.2938 - val_mrcnn_bbox_loss: 0.4661 - val_mrcnn_mask_loss: 0.4772\n",
      "Epoch 18/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 2.0583 - rpn_class_loss: 0.2575 - rpn_bbox_loss: 0.7736 - mrcnn_class_loss: 0.3103 - mrcnn_bbox_loss: 0.3406 - mrcnn_mask_loss: 0.3761 - val_loss: 2.7911 - val_rpn_class_loss: 0.3749 - val_rpn_bbox_loss: 0.9570 - val_mrcnn_class_loss: 0.3820 - val_mrcnn_bbox_loss: 0.5092 - val_mrcnn_mask_loss: 0.5680\n",
      "Epoch 19/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 2.3473 - rpn_class_loss: 0.2660 - rpn_bbox_loss: 1.0057 - mrcnn_class_loss: 0.3240 - mrcnn_bbox_loss: 0.3593 - mrcnn_mask_loss: 0.3922 - val_loss: 2.5579 - val_rpn_class_loss: 0.3436 - val_rpn_bbox_loss: 1.0329 - val_mrcnn_class_loss: 0.2712 - val_mrcnn_bbox_loss: 0.4660 - val_mrcnn_mask_loss: 0.4442\n",
      "Epoch 20/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 2.1479 - rpn_class_loss: 0.2570 - rpn_bbox_loss: 0.8657 - mrcnn_class_loss: 0.3097 - mrcnn_bbox_loss: 0.3429 - mrcnn_mask_loss: 0.3726 - val_loss: 2.8610 - val_rpn_class_loss: 0.3949 - val_rpn_bbox_loss: 1.1512 - val_mrcnn_class_loss: 0.3293 - val_mrcnn_bbox_loss: 0.4817 - val_mrcnn_mask_loss: 0.5039\n",
      "Epoch 21/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 2.1625 - rpn_class_loss: 0.2549 - rpn_bbox_loss: 0.8807 - mrcnn_class_loss: 0.3016 - mrcnn_bbox_loss: 0.3479 - mrcnn_mask_loss: 0.3774 - val_loss: 2.6507 - val_rpn_class_loss: 0.3249 - val_rpn_bbox_loss: 1.0917 - val_mrcnn_class_loss: 0.3138 - val_mrcnn_bbox_loss: 0.4438 - val_mrcnn_mask_loss: 0.4764\n",
      "Epoch 22/80\n",
      "180/180 [==============================] - 231s 1s/step - loss: 2.1311 - rpn_class_loss: 0.2632 - rpn_bbox_loss: 0.8736 - mrcnn_class_loss: 0.2966 - mrcnn_bbox_loss: 0.3291 - mrcnn_mask_loss: 0.3685 - val_loss: 2.6453 - val_rpn_class_loss: 0.3614 - val_rpn_bbox_loss: 1.1013 - val_mrcnn_class_loss: 0.2704 - val_mrcnn_bbox_loss: 0.4479 - val_mrcnn_mask_loss: 0.4642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/80\n",
      "180/180 [==============================] - 232s 1s/step - loss: 2.0797 - rpn_class_loss: 0.2389 - rpn_bbox_loss: 0.8591 - mrcnn_class_loss: 0.2853 - mrcnn_bbox_loss: 0.3307 - mrcnn_mask_loss: 0.3658 - val_loss: 2.6840 - val_rpn_class_loss: 0.3440 - val_rpn_bbox_loss: 1.1922 - val_mrcnn_class_loss: 0.2098 - val_mrcnn_bbox_loss: 0.4733 - val_mrcnn_mask_loss: 0.4648\n",
      "Epoch 24/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 2.1009 - rpn_class_loss: 0.2336 - rpn_bbox_loss: 0.8728 - mrcnn_class_loss: 0.3043 - mrcnn_bbox_loss: 0.3247 - mrcnn_mask_loss: 0.3654 - val_loss: 2.4940 - val_rpn_class_loss: 0.3490 - val_rpn_bbox_loss: 0.9723 - val_mrcnn_class_loss: 0.2863 - val_mrcnn_bbox_loss: 0.4303 - val_mrcnn_mask_loss: 0.4561\n",
      "Epoch 25/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 2.0984 - rpn_class_loss: 0.2460 - rpn_bbox_loss: 0.8662 - mrcnn_class_loss: 0.2963 - mrcnn_bbox_loss: 0.3290 - mrcnn_mask_loss: 0.3610 - val_loss: 2.5445 - val_rpn_class_loss: 0.3029 - val_rpn_bbox_loss: 1.0516 - val_mrcnn_class_loss: 0.2064 - val_mrcnn_bbox_loss: 0.4682 - val_mrcnn_mask_loss: 0.5154\n",
      "Epoch 26/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 1.9368 - rpn_class_loss: 0.2158 - rpn_bbox_loss: 0.7392 - mrcnn_class_loss: 0.2891 - mrcnn_bbox_loss: 0.3265 - mrcnn_mask_loss: 0.3661 - val_loss: 3.0243 - val_rpn_class_loss: 0.4099 - val_rpn_bbox_loss: 1.4775 - val_mrcnn_class_loss: 0.2707 - val_mrcnn_bbox_loss: 0.4280 - val_mrcnn_mask_loss: 0.4381\n",
      "Epoch 27/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 2.2037 - rpn_class_loss: 0.2541 - rpn_bbox_loss: 0.9895 - mrcnn_class_loss: 0.2837 - mrcnn_bbox_loss: 0.3227 - mrcnn_mask_loss: 0.3537 - val_loss: 2.0755 - val_rpn_class_loss: 0.2704 - val_rpn_bbox_loss: 0.7401 - val_mrcnn_class_loss: 0.2057 - val_mrcnn_bbox_loss: 0.4087 - val_mrcnn_mask_loss: 0.4506\n",
      "Epoch 28/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.9957 - rpn_class_loss: 0.2199 - rpn_bbox_loss: 0.7929 - mrcnn_class_loss: 0.2964 - mrcnn_bbox_loss: 0.3256 - mrcnn_mask_loss: 0.3609 - val_loss: 2.4610 - val_rpn_class_loss: 0.3154 - val_rpn_bbox_loss: 0.8988 - val_mrcnn_class_loss: 0.3092 - val_mrcnn_bbox_loss: 0.4578 - val_mrcnn_mask_loss: 0.4798\n",
      "Epoch 29/80\n",
      "180/180 [==============================] - 234s 1s/step - loss: 2.1281 - rpn_class_loss: 0.2527 - rpn_bbox_loss: 0.9099 - mrcnn_class_loss: 0.2913 - mrcnn_bbox_loss: 0.3143 - mrcnn_mask_loss: 0.3598 - val_loss: 2.4274 - val_rpn_class_loss: 0.3411 - val_rpn_bbox_loss: 1.0116 - val_mrcnn_class_loss: 0.2556 - val_mrcnn_bbox_loss: 0.3957 - val_mrcnn_mask_loss: 0.4233\n",
      "Epoch 30/80\n",
      "180/180 [==============================] - 233s 1s/step - loss: 1.8260 - rpn_class_loss: 0.2001 - rpn_bbox_loss: 0.6432 - mrcnn_class_loss: 0.3172 - mrcnn_bbox_loss: 0.3121 - mrcnn_mask_loss: 0.3534 - val_loss: 2.4854 - val_rpn_class_loss: 0.2811 - val_rpn_bbox_loss: 1.0361 - val_mrcnn_class_loss: 0.3157 - val_mrcnn_bbox_loss: 0.4107 - val_mrcnn_mask_loss: 0.4418\n",
      "Epoch 31/80\n",
      "180/180 [==============================] - 243s 1s/step - loss: 2.0251 - rpn_class_loss: 0.2305 - rpn_bbox_loss: 0.8236 - mrcnn_class_loss: 0.2977 - mrcnn_bbox_loss: 0.3200 - mrcnn_mask_loss: 0.3534 - val_loss: 2.6921 - val_rpn_class_loss: 0.3858 - val_rpn_bbox_loss: 0.9791 - val_mrcnn_class_loss: 0.3929 - val_mrcnn_bbox_loss: 0.4567 - val_mrcnn_mask_loss: 0.4777\n",
      "Epoch 32/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 2.0037 - rpn_class_loss: 0.2388 - rpn_bbox_loss: 0.8174 - mrcnn_class_loss: 0.2730 - mrcnn_bbox_loss: 0.3173 - mrcnn_mask_loss: 0.3571 - val_loss: 2.4476 - val_rpn_class_loss: 0.2415 - val_rpn_bbox_loss: 1.1513 - val_mrcnn_class_loss: 0.1485 - val_mrcnn_bbox_loss: 0.4283 - val_mrcnn_mask_loss: 0.4780\n",
      "Epoch 33/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.9145 - rpn_class_loss: 0.2033 - rpn_bbox_loss: 0.7471 - mrcnn_class_loss: 0.2923 - mrcnn_bbox_loss: 0.3167 - mrcnn_mask_loss: 0.3550 - val_loss: 2.4844 - val_rpn_class_loss: 0.2859 - val_rpn_bbox_loss: 1.0718 - val_mrcnn_class_loss: 0.2859 - val_mrcnn_bbox_loss: 0.4127 - val_mrcnn_mask_loss: 0.4281\n",
      "Epoch 34/80\n",
      "180/180 [==============================] - 239s 1s/step - loss: 1.8224 - rpn_class_loss: 0.2055 - rpn_bbox_loss: 0.6759 - mrcnn_class_loss: 0.2939 - mrcnn_bbox_loss: 0.3039 - mrcnn_mask_loss: 0.3431 - val_loss: 3.0509 - val_rpn_class_loss: 0.4719 - val_rpn_bbox_loss: 1.3234 - val_mrcnn_class_loss: 0.3195 - val_mrcnn_bbox_loss: 0.4500 - val_mrcnn_mask_loss: 0.4861\n",
      "Epoch 35/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.9315 - rpn_class_loss: 0.2040 - rpn_bbox_loss: 0.7506 - mrcnn_class_loss: 0.3061 - mrcnn_bbox_loss: 0.3148 - mrcnn_mask_loss: 0.3560 - val_loss: 2.5578 - val_rpn_class_loss: 0.2939 - val_rpn_bbox_loss: 1.2353 - val_mrcnn_class_loss: 0.1742 - val_mrcnn_bbox_loss: 0.4304 - val_mrcnn_mask_loss: 0.4240\n",
      "Epoch 36/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.9352 - rpn_class_loss: 0.2037 - rpn_bbox_loss: 0.7960 - mrcnn_class_loss: 0.2716 - mrcnn_bbox_loss: 0.3009 - mrcnn_mask_loss: 0.3629 - val_loss: 2.2739 - val_rpn_class_loss: 0.2718 - val_rpn_bbox_loss: 0.8785 - val_mrcnn_class_loss: 0.2334 - val_mrcnn_bbox_loss: 0.4199 - val_mrcnn_mask_loss: 0.4703\n",
      "Epoch 37/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.9606 - rpn_class_loss: 0.2151 - rpn_bbox_loss: 0.8176 - mrcnn_class_loss: 0.3047 - mrcnn_bbox_loss: 0.2884 - mrcnn_mask_loss: 0.3347 - val_loss: 2.4237 - val_rpn_class_loss: 0.2409 - val_rpn_bbox_loss: 1.0231 - val_mrcnn_class_loss: 0.2789 - val_mrcnn_bbox_loss: 0.4235 - val_mrcnn_mask_loss: 0.4574\n",
      "Epoch 38/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 2.0265 - rpn_class_loss: 0.1986 - rpn_bbox_loss: 0.8865 - mrcnn_class_loss: 0.2855 - mrcnn_bbox_loss: 0.3078 - mrcnn_mask_loss: 0.3481 - val_loss: 2.4694 - val_rpn_class_loss: 0.3057 - val_rpn_bbox_loss: 1.0805 - val_mrcnn_class_loss: 0.1552 - val_mrcnn_bbox_loss: 0.4657 - val_mrcnn_mask_loss: 0.4623\n",
      "Epoch 39/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.7988 - rpn_class_loss: 0.1938 - rpn_bbox_loss: 0.7158 - mrcnn_class_loss: 0.2777 - mrcnn_bbox_loss: 0.2822 - mrcnn_mask_loss: 0.3294 - val_loss: 2.3166 - val_rpn_class_loss: 0.2859 - val_rpn_bbox_loss: 0.9712 - val_mrcnn_class_loss: 0.2266 - val_mrcnn_bbox_loss: 0.4075 - val_mrcnn_mask_loss: 0.4254\n",
      "Epoch 40/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.8183 - rpn_class_loss: 0.1898 - rpn_bbox_loss: 0.7102 - mrcnn_class_loss: 0.2897 - mrcnn_bbox_loss: 0.2909 - mrcnn_mask_loss: 0.3376 - val_loss: 2.5720 - val_rpn_class_loss: 0.2913 - val_rpn_bbox_loss: 1.1512 - val_mrcnn_class_loss: 0.2960 - val_mrcnn_bbox_loss: 0.4039 - val_mrcnn_mask_loss: 0.4297\n",
      "Epoch 41/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.7979 - rpn_class_loss: 0.1841 - rpn_bbox_loss: 0.7179 - mrcnn_class_loss: 0.2770 - mrcnn_bbox_loss: 0.2905 - mrcnn_mask_loss: 0.3285 - val_loss: 3.3028 - val_rpn_class_loss: 0.4773 - val_rpn_bbox_loss: 1.5325 - val_mrcnn_class_loss: 0.3675 - val_mrcnn_bbox_loss: 0.4370 - val_mrcnn_mask_loss: 0.4885\n",
      "Epoch 42/80\n",
      "180/180 [==============================] - 243s 1s/step - loss: 2.0160 - rpn_class_loss: 0.2146 - rpn_bbox_loss: 0.8977 - mrcnn_class_loss: 0.2691 - mrcnn_bbox_loss: 0.2964 - mrcnn_mask_loss: 0.3381 - val_loss: 2.3528 - val_rpn_class_loss: 0.3123 - val_rpn_bbox_loss: 0.8838 - val_mrcnn_class_loss: 0.2580 - val_mrcnn_bbox_loss: 0.4384 - val_mrcnn_mask_loss: 0.4603\n",
      "Epoch 43/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.8682 - rpn_class_loss: 0.1927 - rpn_bbox_loss: 0.7477 - mrcnn_class_loss: 0.2895 - mrcnn_bbox_loss: 0.2977 - mrcnn_mask_loss: 0.3407 - val_loss: 2.2843 - val_rpn_class_loss: 0.2704 - val_rpn_bbox_loss: 0.8331 - val_mrcnn_class_loss: 0.3422 - val_mrcnn_bbox_loss: 0.4065 - val_mrcnn_mask_loss: 0.4321\n",
      "Epoch 44/80\n",
      "180/180 [==============================] - 238s 1s/step - loss: 1.7623 - rpn_class_loss: 0.1778 - rpn_bbox_loss: 0.7015 - mrcnn_class_loss: 0.2573 - mrcnn_bbox_loss: 0.2888 - mrcnn_mask_loss: 0.3368 - val_loss: 2.3425 - val_rpn_class_loss: 0.3166 - val_rpn_bbox_loss: 0.8129 - val_mrcnn_class_loss: 0.2973 - val_mrcnn_bbox_loss: 0.4462 - val_mrcnn_mask_loss: 0.4695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 1.7672 - rpn_class_loss: 0.1695 - rpn_bbox_loss: 0.6976 - mrcnn_class_loss: 0.2776 - mrcnn_bbox_loss: 0.2882 - mrcnn_mask_loss: 0.3343 - val_loss: 2.0572 - val_rpn_class_loss: 0.1987 - val_rpn_bbox_loss: 0.7968 - val_mrcnn_class_loss: 0.2526 - val_mrcnn_bbox_loss: 0.3657 - val_mrcnn_mask_loss: 0.4434\n",
      "Epoch 46/80\n",
      "180/180 [==============================] - 238s 1s/step - loss: 1.8969 - rpn_class_loss: 0.2024 - rpn_bbox_loss: 0.8071 - mrcnn_class_loss: 0.2702 - mrcnn_bbox_loss: 0.2836 - mrcnn_mask_loss: 0.3335 - val_loss: 2.3995 - val_rpn_class_loss: 0.2850 - val_rpn_bbox_loss: 1.0118 - val_mrcnn_class_loss: 0.2202 - val_mrcnn_bbox_loss: 0.4175 - val_mrcnn_mask_loss: 0.4650\n",
      "Epoch 47/80\n",
      "180/180 [==============================] - 243s 1s/step - loss: 1.8108 - rpn_class_loss: 0.1841 - rpn_bbox_loss: 0.7061 - mrcnn_class_loss: 0.2963 - mrcnn_bbox_loss: 0.2834 - mrcnn_mask_loss: 0.3409 - val_loss: 2.6733 - val_rpn_class_loss: 0.3320 - val_rpn_bbox_loss: 1.2644 - val_mrcnn_class_loss: 0.2825 - val_mrcnn_bbox_loss: 0.3845 - val_mrcnn_mask_loss: 0.4099\n",
      "Epoch 48/80\n",
      "180/180 [==============================] - 236s 1s/step - loss: 1.8196 - rpn_class_loss: 0.1812 - rpn_bbox_loss: 0.7780 - mrcnn_class_loss: 0.2695 - mrcnn_bbox_loss: 0.2641 - mrcnn_mask_loss: 0.3267 - val_loss: 2.3167 - val_rpn_class_loss: 0.2909 - val_rpn_bbox_loss: 0.8582 - val_mrcnn_class_loss: 0.3005 - val_mrcnn_bbox_loss: 0.4094 - val_mrcnn_mask_loss: 0.4578\n",
      "Epoch 49/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 1.8545 - rpn_class_loss: 0.1973 - rpn_bbox_loss: 0.7741 - mrcnn_class_loss: 0.2845 - mrcnn_bbox_loss: 0.2727 - mrcnn_mask_loss: 0.3258 - val_loss: 2.3882 - val_rpn_class_loss: 0.2694 - val_rpn_bbox_loss: 0.9948 - val_mrcnn_class_loss: 0.2420 - val_mrcnn_bbox_loss: 0.4142 - val_mrcnn_mask_loss: 0.4678\n",
      "Epoch 50/80\n",
      "180/180 [==============================] - 243s 1s/step - loss: 1.8692 - rpn_class_loss: 0.1922 - rpn_bbox_loss: 0.7941 - mrcnn_class_loss: 0.2905 - mrcnn_bbox_loss: 0.2699 - mrcnn_mask_loss: 0.3225 - val_loss: 2.3541 - val_rpn_class_loss: 0.2801 - val_rpn_bbox_loss: 1.0281 - val_mrcnn_class_loss: 0.2393 - val_mrcnn_bbox_loss: 0.4006 - val_mrcnn_mask_loss: 0.4061\n",
      "Epoch 51/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 1.7946 - rpn_class_loss: 0.1764 - rpn_bbox_loss: 0.7793 - mrcnn_class_loss: 0.2557 - mrcnn_bbox_loss: 0.2659 - mrcnn_mask_loss: 0.3173 - val_loss: 2.5411 - val_rpn_class_loss: 0.3585 - val_rpn_bbox_loss: 0.9348 - val_mrcnn_class_loss: 0.2968 - val_mrcnn_bbox_loss: 0.4537 - val_mrcnn_mask_loss: 0.4972\n",
      "Epoch 52/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.7380 - rpn_class_loss: 0.1697 - rpn_bbox_loss: 0.7077 - mrcnn_class_loss: 0.2618 - mrcnn_bbox_loss: 0.2742 - mrcnn_mask_loss: 0.3246 - val_loss: 2.2037 - val_rpn_class_loss: 0.2857 - val_rpn_bbox_loss: 0.8180 - val_mrcnn_class_loss: 0.2518 - val_mrcnn_bbox_loss: 0.4126 - val_mrcnn_mask_loss: 0.4356\n",
      "Epoch 53/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 1.8459 - rpn_class_loss: 0.1794 - rpn_bbox_loss: 0.8087 - mrcnn_class_loss: 0.2713 - mrcnn_bbox_loss: 0.2659 - mrcnn_mask_loss: 0.3205 - val_loss: 2.2167 - val_rpn_class_loss: 0.2457 - val_rpn_bbox_loss: 0.9383 - val_mrcnn_class_loss: 0.2576 - val_mrcnn_bbox_loss: 0.3582 - val_mrcnn_mask_loss: 0.4170\n",
      "Epoch 54/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 1.6815 - rpn_class_loss: 0.1689 - rpn_bbox_loss: 0.6739 - mrcnn_class_loss: 0.2597 - mrcnn_bbox_loss: 0.2635 - mrcnn_mask_loss: 0.3155 - val_loss: 2.0228 - val_rpn_class_loss: 0.2480 - val_rpn_bbox_loss: 0.6579 - val_mrcnn_class_loss: 0.2266 - val_mrcnn_bbox_loss: 0.4443 - val_mrcnn_mask_loss: 0.4459\n",
      "Epoch 55/80\n",
      "180/180 [==============================] - 238s 1s/step - loss: 1.7938 - rpn_class_loss: 0.1762 - rpn_bbox_loss: 0.7536 - mrcnn_class_loss: 0.2752 - mrcnn_bbox_loss: 0.2665 - mrcnn_mask_loss: 0.3223 - val_loss: 2.4267 - val_rpn_class_loss: 0.3063 - val_rpn_bbox_loss: 0.9917 - val_mrcnn_class_loss: 0.2430 - val_mrcnn_bbox_loss: 0.4460 - val_mrcnn_mask_loss: 0.4396\n",
      "Epoch 56/80\n",
      "180/180 [==============================] - 232s 1s/step - loss: 1.6855 - rpn_class_loss: 0.1791 - rpn_bbox_loss: 0.6484 - mrcnn_class_loss: 0.2816 - mrcnn_bbox_loss: 0.2583 - mrcnn_mask_loss: 0.3180 - val_loss: 2.3361 - val_rpn_class_loss: 0.3071 - val_rpn_bbox_loss: 0.9565 - val_mrcnn_class_loss: 0.2332 - val_mrcnn_bbox_loss: 0.4052 - val_mrcnn_mask_loss: 0.4340\n",
      "Epoch 57/80\n",
      "180/180 [==============================] - 239s 1s/step - loss: 1.9198 - rpn_class_loss: 0.1636 - rpn_bbox_loss: 0.8987 - mrcnn_class_loss: 0.2618 - mrcnn_bbox_loss: 0.2728 - mrcnn_mask_loss: 0.3228 - val_loss: 2.4532 - val_rpn_class_loss: 0.3726 - val_rpn_bbox_loss: 0.9553 - val_mrcnn_class_loss: 0.2298 - val_mrcnn_bbox_loss: 0.4310 - val_mrcnn_mask_loss: 0.4645\n",
      "Epoch 58/80\n",
      "180/180 [==============================] - 242s 1s/step - loss: 1.8576 - rpn_class_loss: 0.1726 - rpn_bbox_loss: 0.8792 - mrcnn_class_loss: 0.2396 - mrcnn_bbox_loss: 0.2562 - mrcnn_mask_loss: 0.3099 - val_loss: 2.6019 - val_rpn_class_loss: 0.3286 - val_rpn_bbox_loss: 1.1955 - val_mrcnn_class_loss: 0.2633 - val_mrcnn_bbox_loss: 0.3921 - val_mrcnn_mask_loss: 0.4223\n",
      "Epoch 59/80\n",
      "180/180 [==============================] - 244s 1s/step - loss: 1.7422 - rpn_class_loss: 0.1761 - rpn_bbox_loss: 0.7089 - mrcnn_class_loss: 0.2649 - mrcnn_bbox_loss: 0.2720 - mrcnn_mask_loss: 0.3202 - val_loss: 2.3277 - val_rpn_class_loss: 0.3088 - val_rpn_bbox_loss: 0.8573 - val_mrcnn_class_loss: 0.3377 - val_mrcnn_bbox_loss: 0.3977 - val_mrcnn_mask_loss: 0.4262\n",
      "Epoch 60/80\n",
      "180/180 [==============================] - 238s 1s/step - loss: 1.7996 - rpn_class_loss: 0.1646 - rpn_bbox_loss: 0.7792 - mrcnn_class_loss: 0.2675 - mrcnn_bbox_loss: 0.2671 - mrcnn_mask_loss: 0.3212 - val_loss: 2.1438 - val_rpn_class_loss: 0.2576 - val_rpn_bbox_loss: 0.6986 - val_mrcnn_class_loss: 0.3418 - val_mrcnn_bbox_loss: 0.3954 - val_mrcnn_mask_loss: 0.4504\n",
      "Epoch 61/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.6629 - rpn_class_loss: 0.1689 - rpn_bbox_loss: 0.6654 - mrcnn_class_loss: 0.2509 - mrcnn_bbox_loss: 0.2608 - mrcnn_mask_loss: 0.3169 - val_loss: 2.3270 - val_rpn_class_loss: 0.2679 - val_rpn_bbox_loss: 0.8924 - val_mrcnn_class_loss: 0.3471 - val_mrcnn_bbox_loss: 0.3930 - val_mrcnn_mask_loss: 0.4266\n",
      "Epoch 62/80\n",
      "180/180 [==============================] - 232s 1s/step - loss: 1.7180 - rpn_class_loss: 0.1564 - rpn_bbox_loss: 0.7468 - mrcnn_class_loss: 0.2489 - mrcnn_bbox_loss: 0.2565 - mrcnn_mask_loss: 0.3093 - val_loss: 2.2280 - val_rpn_class_loss: 0.2476 - val_rpn_bbox_loss: 0.9365 - val_mrcnn_class_loss: 0.2813 - val_mrcnn_bbox_loss: 0.3476 - val_mrcnn_mask_loss: 0.4148\n",
      "Epoch 63/80\n",
      "180/180 [==============================] - 238s 1s/step - loss: 1.6497 - rpn_class_loss: 0.1672 - rpn_bbox_loss: 0.6679 - mrcnn_class_loss: 0.2566 - mrcnn_bbox_loss: 0.2513 - mrcnn_mask_loss: 0.3066 - val_loss: 1.9312 - val_rpn_class_loss: 0.2376 - val_rpn_bbox_loss: 0.6872 - val_mrcnn_class_loss: 0.2135 - val_mrcnn_bbox_loss: 0.3780 - val_mrcnn_mask_loss: 0.4149\n",
      "Epoch 64/80\n",
      "180/180 [==============================] - 233s 1s/step - loss: 1.7536 - rpn_class_loss: 0.1557 - rpn_bbox_loss: 0.7784 - mrcnn_class_loss: 0.2516 - mrcnn_bbox_loss: 0.2552 - mrcnn_mask_loss: 0.3126 - val_loss: 2.0832 - val_rpn_class_loss: 0.2351 - val_rpn_bbox_loss: 0.8039 - val_mrcnn_class_loss: 0.2527 - val_mrcnn_bbox_loss: 0.3728 - val_mrcnn_mask_loss: 0.4187\n",
      "Epoch 65/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.6165 - rpn_class_loss: 0.1544 - rpn_bbox_loss: 0.6420 - mrcnn_class_loss: 0.2520 - mrcnn_bbox_loss: 0.2521 - mrcnn_mask_loss: 0.3159 - val_loss: 2.5981 - val_rpn_class_loss: 0.4694 - val_rpn_bbox_loss: 1.1192 - val_mrcnn_class_loss: 0.2544 - val_mrcnn_bbox_loss: 0.3524 - val_mrcnn_mask_loss: 0.4027\n",
      "Epoch 66/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 1.6676 - rpn_class_loss: 0.1636 - rpn_bbox_loss: 0.7087 - mrcnn_class_loss: 0.2508 - mrcnn_bbox_loss: 0.2450 - mrcnn_mask_loss: 0.2996 - val_loss: 2.2081 - val_rpn_class_loss: 0.2260 - val_rpn_bbox_loss: 0.9127 - val_mrcnn_class_loss: 0.2532 - val_mrcnn_bbox_loss: 0.3822 - val_mrcnn_mask_loss: 0.4340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/80\n",
      "180/180 [==============================] - 235s 1s/step - loss: 1.8472 - rpn_class_loss: 0.1885 - rpn_bbox_loss: 0.8501 - mrcnn_class_loss: 0.2293 - mrcnn_bbox_loss: 0.2625 - mrcnn_mask_loss: 0.3167 - val_loss: 2.5372 - val_rpn_class_loss: 0.3245 - val_rpn_bbox_loss: 1.1161 - val_mrcnn_class_loss: 0.2582 - val_mrcnn_bbox_loss: 0.4051 - val_mrcnn_mask_loss: 0.4332\n",
      "Epoch 68/80\n",
      "180/180 [==============================] - 234s 1s/step - loss: 1.6300 - rpn_class_loss: 0.1551 - rpn_bbox_loss: 0.6788 - mrcnn_class_loss: 0.2548 - mrcnn_bbox_loss: 0.2389 - mrcnn_mask_loss: 0.3023 - val_loss: 2.3938 - val_rpn_class_loss: 0.2959 - val_rpn_bbox_loss: 0.9789 - val_mrcnn_class_loss: 0.2810 - val_mrcnn_bbox_loss: 0.3890 - val_mrcnn_mask_loss: 0.4491\n",
      "Epoch 69/80\n",
      "180/180 [==============================] - 241s 1s/step - loss: 1.7700 - rpn_class_loss: 0.1706 - rpn_bbox_loss: 0.7860 - mrcnn_class_loss: 0.2472 - mrcnn_bbox_loss: 0.2563 - mrcnn_mask_loss: 0.3098 - val_loss: 2.3985 - val_rpn_class_loss: 0.3681 - val_rpn_bbox_loss: 0.9610 - val_mrcnn_class_loss: 0.3003 - val_mrcnn_bbox_loss: 0.3792 - val_mrcnn_mask_loss: 0.3900\n",
      "Epoch 70/80\n",
      "180/180 [==============================] - 236s 1s/step - loss: 1.5876 - rpn_class_loss: 0.1622 - rpn_bbox_loss: 0.6244 - mrcnn_class_loss: 0.2568 - mrcnn_bbox_loss: 0.2429 - mrcnn_mask_loss: 0.3012 - val_loss: 2.5167 - val_rpn_class_loss: 0.2994 - val_rpn_bbox_loss: 1.1758 - val_mrcnn_class_loss: 0.2304 - val_mrcnn_bbox_loss: 0.3869 - val_mrcnn_mask_loss: 0.4242\n",
      "Epoch 71/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 1.8275 - rpn_class_loss: 0.1664 - rpn_bbox_loss: 0.8540 - mrcnn_class_loss: 0.2478 - mrcnn_bbox_loss: 0.2482 - mrcnn_mask_loss: 0.3111 - val_loss: 2.1115 - val_rpn_class_loss: 0.2520 - val_rpn_bbox_loss: 0.8252 - val_mrcnn_class_loss: 0.2218 - val_mrcnn_bbox_loss: 0.3899 - val_mrcnn_mask_loss: 0.4226\n",
      "Epoch 72/80\n",
      "180/180 [==============================] - 233s 1s/step - loss: 1.6641 - rpn_class_loss: 0.1607 - rpn_bbox_loss: 0.6395 - mrcnn_class_loss: 0.2698 - mrcnn_bbox_loss: 0.2708 - mrcnn_mask_loss: 0.3233 - val_loss: 2.2771 - val_rpn_class_loss: 0.3089 - val_rpn_bbox_loss: 0.9142 - val_mrcnn_class_loss: 0.2381 - val_mrcnn_bbox_loss: 0.3773 - val_mrcnn_mask_loss: 0.4387\n",
      "Epoch 73/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.7840 - rpn_class_loss: 0.1552 - rpn_bbox_loss: 0.8058 - mrcnn_class_loss: 0.2558 - mrcnn_bbox_loss: 0.2552 - mrcnn_mask_loss: 0.3120 - val_loss: 2.0069 - val_rpn_class_loss: 0.1970 - val_rpn_bbox_loss: 0.8156 - val_mrcnn_class_loss: 0.1966 - val_mrcnn_bbox_loss: 0.3778 - val_mrcnn_mask_loss: 0.4199\n",
      "Epoch 74/80\n",
      "180/180 [==============================] - 231s 1s/step - loss: 1.6843 - rpn_class_loss: 0.1542 - rpn_bbox_loss: 0.7114 - mrcnn_class_loss: 0.2572 - mrcnn_bbox_loss: 0.2505 - mrcnn_mask_loss: 0.3110 - val_loss: 2.0389 - val_rpn_class_loss: 0.2659 - val_rpn_bbox_loss: 0.7329 - val_mrcnn_class_loss: 0.2316 - val_mrcnn_bbox_loss: 0.3720 - val_mrcnn_mask_loss: 0.4364\n",
      "Epoch 75/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.7047 - rpn_class_loss: 0.1513 - rpn_bbox_loss: 0.7576 - mrcnn_class_loss: 0.2562 - mrcnn_bbox_loss: 0.2385 - mrcnn_mask_loss: 0.3011 - val_loss: 2.0614 - val_rpn_class_loss: 0.2236 - val_rpn_bbox_loss: 0.8826 - val_mrcnn_class_loss: 0.2269 - val_mrcnn_bbox_loss: 0.3568 - val_mrcnn_mask_loss: 0.3715\n",
      "Epoch 76/80\n",
      "180/180 [==============================] - 240s 1s/step - loss: 1.7949 - rpn_class_loss: 0.1693 - rpn_bbox_loss: 0.8162 - mrcnn_class_loss: 0.2583 - mrcnn_bbox_loss: 0.2455 - mrcnn_mask_loss: 0.3057 - val_loss: 1.8831 - val_rpn_class_loss: 0.1917 - val_rpn_bbox_loss: 0.6535 - val_mrcnn_class_loss: 0.2273 - val_mrcnn_bbox_loss: 0.3846 - val_mrcnn_mask_loss: 0.4259\n",
      "Epoch 77/80\n",
      "180/180 [==============================] - 237s 1s/step - loss: 1.5694 - rpn_class_loss: 0.1545 - rpn_bbox_loss: 0.6186 - mrcnn_class_loss: 0.2510 - mrcnn_bbox_loss: 0.2405 - mrcnn_mask_loss: 0.3048 - val_loss: 2.3053 - val_rpn_class_loss: 0.2697 - val_rpn_bbox_loss: 1.0026 - val_mrcnn_class_loss: 0.2225 - val_mrcnn_bbox_loss: 0.3759 - val_mrcnn_mask_loss: 0.4345\n",
      "Epoch 78/80\n",
      "180/180 [==============================] - 233s 1s/step - loss: 1.6847 - rpn_class_loss: 0.1441 - rpn_bbox_loss: 0.7216 - mrcnn_class_loss: 0.2576 - mrcnn_bbox_loss: 0.2561 - mrcnn_mask_loss: 0.3052 - val_loss: 2.4124 - val_rpn_class_loss: 0.3262 - val_rpn_bbox_loss: 1.0235 - val_mrcnn_class_loss: 0.2830 - val_mrcnn_bbox_loss: 0.3698 - val_mrcnn_mask_loss: 0.4098\n",
      "Epoch 79/80\n",
      "180/180 [==============================] - 213s 1s/step - loss: 1.8093 - rpn_class_loss: 0.1598 - rpn_bbox_loss: 0.8439 - mrcnn_class_loss: 0.2604 - mrcnn_bbox_loss: 0.2452 - mrcnn_mask_loss: 0.2999 - val_loss: 2.3975 - val_rpn_class_loss: 0.3091 - val_rpn_bbox_loss: 1.0748 - val_mrcnn_class_loss: 0.2271 - val_mrcnn_bbox_loss: 0.3701 - val_mrcnn_mask_loss: 0.4163\n",
      "Epoch 80/80\n",
      "180/180 [==============================] - 207s 1s/step - loss: 1.6330 - rpn_class_loss: 0.1645 - rpn_bbox_loss: 0.7012 - mrcnn_class_loss: 0.2335 - mrcnn_bbox_loss: 0.2350 - mrcnn_mask_loss: 0.2987 - val_loss: 2.2647 - val_rpn_class_loss: 0.2766 - val_rpn_bbox_loss: 0.9345 - val_mrcnn_class_loss: 0.2683 - val_mrcnn_bbox_loss: 0.3684 - val_mrcnn_mask_loss: 0.4169\n",
      "Training took 324.42 minutes\n"
     ]
    }
   ],
   "source": [
    "start_train = time.time()\n",
    "model.train(dataset_train, dataset_val, learning_rate = TrainConfig().LEARNING_RATE, epochs = NUM_EPOCHS, layers = 'all')\n",
    "end_train = time.time()\n",
    "minutes = round((end_train - start_train) / 60, 2)\n",
    "print(f'Training took {minutes} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include evaluation scripts in training script so that the kernel does not have to be reloaded. Eases the process of rapidly training and evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO related libraries\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "from samples.coco import coco\n",
    "from samples.coco.coco import evaluate_coco\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# MaskRCNN libraries\n",
    "import mrcnn.model as modellib\n",
    "import mrcnn.utils as utils\n",
    "import mrcnn.visualize as visualize\n",
    "\n",
    "# Misc\n",
    "import os\n",
    "import skimage.io\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tnrange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare evaluation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalConfig(coco.CocoConfig):\n",
    "    \"\"\" Configuration for evaluation \"\"\"\n",
    "    \n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = MODEL_NAME\n",
    "    \n",
    "    # How many GPUs\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    # How many images per gpu\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + NUM_CLASSES  # background + other classes\n",
    "    \n",
    "    IMAGE_MIN_DIM = 1152\n",
    "    IMAGE_MAX_DIM = 1280\n",
    "    \n",
    "    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n",
    "    BACKBONE = 'resnet101'\n",
    "\n",
    "    # To be honest, I haven't taken the time to figure out what these do\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    \n",
    "    # Changed to 512 because that's how many the original MaskRCNN paper used\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "    MAX_GT_INSTANCES = 114\n",
    "    POST_NMS_ROIS_INFERENCE = 1000 \n",
    "    POST_NMS_ROIS_TRAINING = 2000 \n",
    "    \n",
    "    DETECTION_MAX_INSTANCES = 114\n",
    "    DETECTION_MIN_CONFIDENCE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        114\n",
      "DETECTION_MIN_CONFIDENCE       0.1\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1280\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  1152\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1280 1280    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               114\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           model_2\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EvalConfig().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build class to load ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(utils.Dataset):\n",
    "    def load_coco_gt(self, annotations_file, dataset_dir):\n",
    "        \"\"\"Load a COCO styled ground truth dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create COCO object\n",
    "        coco = COCO(annotations_file)\n",
    "\n",
    "        # Load all classes\n",
    "        class_ids = sorted(coco.getCatIds())\n",
    "\n",
    "        # Load all images\n",
    "        image_ids = list(coco.imgs.keys())\n",
    "\n",
    "        # Add classes\n",
    "        for i in class_ids:\n",
    "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
    "\n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"coco\", image_id = i,\n",
    "                path = os.path.join(dataset_dir, coco.imgs[i]['file_name']),\n",
    "                width = coco.imgs[i][\"width\"],\n",
    "                height = coco.imgs[i][\"height\"],\n",
    "                annotations = coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds = [i], catIds = class_ids, iscrowd=None)))\n",
    "        \n",
    "        return coco\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"coco\":\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return a link to the image in the COCO Website.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"coco\":\n",
    "            return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\n",
    "        else:\n",
    "            super(CocoDataset, self).image_reference(image_id)\n",
    "\n",
    "    # The following two functions are from pycocotools with a few changes.\n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MaskRCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 02:20:35.781523 139871543367488 deprecation_wrapper.py:119] From /home/venv/src/mask-rcnn/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "W0806 02:20:35.898080 139871543367488 deprecation.py:323] From /home/venv/src/mask-rcnn/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the model in inference mode\n",
    "model = modellib.MaskRCNN(mode = \"inference\", config = EvalConfig(), model_dir = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights from last trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-starting from epoch 80\n"
     ]
    }
   ],
   "source": [
    "# Load weights by name\n",
    "model.load_weights(model.find_last(), by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Relative path to ground truth annotations JSON file\n",
    "GT_ANNOTATIONS_FILE = \"datasets/Downtown_Sliced/test/annotations_split.json\"\n",
    "\n",
    "# Relative path to images associated with ground truth JSON file\n",
    "GT_DATASET_DIR = \"datasets/Downtown_Sliced/test/images\"\n",
    "\n",
    "dataset_val = CocoDataset()\n",
    "coco = dataset_val.load_coco_gt(annotations_file = GT_ANNOTATIONS_FILE, dataset_dir = GT_DATASET_DIR)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model with COCO test\n",
    "### If your results come back as a bunch of zeros, check to make sure that the \"width\" and \"height\" tag in your COCO dataset are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_coco(model, dataset_val, coco, \"segm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating mAP as per example in train_shapes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f181ef86f524c2480334eb3152a9493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Processing images in dataset...', max=284, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mAP:  0.3197467292721128\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, len(dataset_val.image_ids))\n",
    "\n",
    "# Instanciate arrays to create our metrics\n",
    "APs = []\n",
    "precisions_arr = []\n",
    "recalls_arr = []\n",
    "overlaps_arr = []\n",
    "class_ids_arr = []\n",
    "scores_arr = []\n",
    "\n",
    "for id in tnrange(len(image_ids), desc = \"Processing images in dataset...\"):\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, EvalConfig(),\n",
    "                               image_ids[id], use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, EvalConfig()), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    # Append AP to AP array\n",
    "    APs.append(AP)\n",
    "    \n",
    "    # Append precisions\n",
    "    for precision in precisions:\n",
    "        precisions_arr.append(precision)\n",
    "    \n",
    "    # Append recalls\n",
    "    for recall in recalls:\n",
    "        recalls_arr.append(recall)\n",
    "    \n",
    "    # Append overlaps\n",
    "    for overlap in overlaps:\n",
    "        overlaps_arr.append(overlap)\n",
    "    \n",
    "    # Append class_ids\n",
    "    for class_id in r[\"class_ids\"]:\n",
    "        class_ids_arr.append(class_id)\n",
    "    \n",
    "    # Append scores \n",
    "    for score in r[\"scores\"]:\n",
    "        scores_arr.append(score)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot precision recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize.plot_precision_recall(AP, precisions, recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
